{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVqYD8imgkQx"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/eval-course/blob/main/notebooks/chapter_01_0.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{eval-course-01} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3JaZVB3gkQy"
      },
      "source": [
        "# Chapter 1: Introduction to LLM Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwlnNEYOgkQz"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EvJoMsJygkQz",
        "outputId": "a36a2fbb-7097-421d-e740-b4e1402355ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'eval-course'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Counting objects: 100% (204/204), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 204 (delta 120), reused 131 (delta 55), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (204/204), 1.16 MiB | 8.87 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n",
            "/content/eval-course\n",
            "Collecting uv\n",
            "  Downloading uv-0.5.16-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.5.16-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.5.16\n",
            "\u001b[2mUsing Python 3.10.12 environment at /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 229ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 60.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 76.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 92.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 108.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 140.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 140.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 140.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 156.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 172.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 188.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 220.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 620.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 652.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1004.63 KiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.15 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.03 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 2.47 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 2.47 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 2.50 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 3.75 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 5.51 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 6.00 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 6.50 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 7.65 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 9.67 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 12.07 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 14.77 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mnumpy     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.10 MiB/17.40 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/2)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 1.53s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 54ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 60ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.14.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.11.4\u001b[0m\n",
            "/content/eval-course/notebooks\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone --branch main https://github.com/wandb/eval-course\n",
        "    %cd eval-course\n",
        "    !pip install uv\n",
        "    !uv pip install --system --quiet -r requirements.txt\n",
        "    !uv pip install --system scipy==1.11.4\n",
        "    %cd notebooks\n",
        "else:\n",
        "    print(\"Not running in Google Colab. Skipping git clone and pip install commands.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WMMQYNkEgkQ0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    in_jupyter = True\n",
        "except ImportError:\n",
        "    in_jupyter = False\n",
        "if in_jupyter:\n",
        "    import nest_asyncio\n",
        "\n",
        "    nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U4ItJceYgkQ0"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "\n",
        "import weave\n",
        "from set_env import set_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "42sSU8nRgkQ1",
        "outputId": "1f9b0af3-4c17-4730-d62f-498c4aedd5ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-01-10 09:19:57.344\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mset_env.set_env\u001b[0m:\u001b[36mset_env\u001b[0m:\u001b[36m109\u001b[0m - \u001b[33m\u001b[1m\n",
            "        Unable to set GOOGLE_API_KEY=GOOGLE_API_KEY,\n",
            "        not in colab or Secrets not set, not kaggle\n",
            "        or Secrets not set, no .env/dotenv/env file\n",
            "        in the current working dir or parent dirs.\u001b[0m\n",
            "\u001b[32m2025-01-10 09:19:59.088\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mset_env.set_env\u001b[0m:\u001b[36mset_env\u001b[0m:\u001b[36m109\u001b[0m - \u001b[33m\u001b[1m\n",
            "        Unable to set WANDB_API_KEY=WANDB_API_KEY,\n",
            "        not in colab or Secrets not set, not kaggle\n",
            "        or Secrets not set, no .env/dotenv/env file\n",
            "        in the current working dir or parent dirs.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Env set!\n"
          ]
        }
      ],
      "source": [
        "set_env(\"GOOGLE_API_KEY\")\n",
        "set_env(\"WANDB_API_KEY\")\n",
        "print(\"Env set!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!vi .env\n"
      ],
      "metadata": {
        "id": "3Y4zFDzchY5S",
        "outputId": "cdf71ceb-818a-4fb1-da50-82497bc1dd01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b7\u001b[?47h\u001b[>4;2m\u001b[?1h\u001b=\u001b[?2004h\u001b[?1004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[22;2t\u001b[22;1t\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\".env\" [New]\u001b[2;1H▽\u001b[6n\u001b[2;1H  \u001b[3;1H\u001bPzz\u001b\\\u001b[0%m\u001b[6n\u001b[3;1H           \u001b[1;1H\u001b[>c\u001b]10;?\u0007\u001b]11;?\u0007\u001b[2;1H\u001b[1m\u001b[34m~                                                                                                   \u001b[3;1H~                                                                                                   \u001b[4;1H~                                                                                                   \u001b[5;1H~                                                                                                   \u001b[6;1H~                                                                                                   \u001b[7;1H~                                                                                                   \u001b[8;1H~                                                                                                   \u001b[9;1H~                                                                                                   \u001b[10;1H~                                                                                                   \u001b[11;1H~                                                                                                   \u001b[12;1H~                                                                                                   \u001b[13;1H~                                                                                                   \u001b[14;1H~                                                                                                   \u001b[15;1H~                                                                                                   \u001b[16;1H~                                                                                                   \u001b[17;1H~                                                                                                   \u001b[18;1H~                                                                                                   \u001b[19;1H~                                                                                                   \u001b[20;1H~                                                                                                   \u001b[21;1H~                                                                                                   \u001b[22;1H~                                                                                                   \u001b[23;1H~                                                                                                   \u001b[m\u001b[24;83H0,0-1\u001b[9CAll\u001b[1;1H\u001b[?25h\u001b[24;1H\u001b[?1004l\u001b[?2004l\u001b[?1l\u001b>\u001b[>4;m\u001b[2J\u001b[?47l\u001b8Vim: Caught deadly signal TERM\n",
            "Vim: Finished.\n",
            "\u001b[24;1H\u001b[23;2t\u001b[23;1t\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3n4KNwy5gkQ1"
      },
      "outputs": [],
      "source": [
        "from utils.config import ENTITY, MODEL, MODEL_CLIENT, WEAVE_PROJECT\n",
        "from utils.evals import calculate_kappa_scores, get_evaluation_predictions\n",
        "from utils.llm_client import LLMClient\n",
        "from utils.prompts import (\n",
        "    MedicalPrivacyJudgement,\n",
        "    MedicalTaskScoreJudgement,\n",
        "    medical_privacy_judge_prompt,\n",
        "    medical_privacy_system_prompt,\n",
        "    medical_system_prompt,\n",
        "    medical_task,\n",
        "    medical_task_score_prompt,\n",
        "    medical_task_score_system_prompt,\n",
        ")\n",
        "from utils.render import display_prompt, print_dialogue_data\n",
        "from utils.deserialize import MainCriteria, deserialize_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkRjqD1tgkQ2"
      },
      "source": [
        "## Understanding Medical Data Extraction Evaluation\n",
        "\n",
        "### The Task: What Are We Trying to Do?\n",
        "\n",
        "#### Raw Data Format\n",
        "Medical conversations are messy and unstructured. Looking at our example data:\n",
        "\n",
        "- Back-and-forth conversation between doctor and patient\n",
        "- Contains personal details, small talk, and medical information mixed together\n",
        "- Informal language (\"hey\", \"mm-hmm\", \"yeah\")\n",
        "- Important details scattered throughout\n",
        "\n",
        "#### Extraction Goals\n",
        "The LLM needs to:\n",
        "1. Find relevant information\n",
        "2. Ignore irrelevant details\n",
        "3. Standardize the format\n",
        "4. Protect patient privacy\n",
        "5. Maintain medical accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcsVrYLMgkQ2"
      },
      "outputs": [],
      "source": [
        "if ENTITY is not None:\n",
        "    weave_client = weave.init(f\"{ENTITY}/{WEAVE_PROJECT}\")\n",
        "else:\n",
        "    weave_client = weave.init(f\"{WEAVE_PROJECT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahGVILtjgkQ2"
      },
      "outputs": [],
      "source": [
        "display_prompt(medical_system_prompt)\n",
        "display_prompt(medical_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0JcVxAggkQ2"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/wandb/eval-course/blob/main/notebooks/media/medical_chatbot.png?raw=1\" width=\"250\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnRyt3CmgkQ2"
      },
      "outputs": [],
      "source": [
        "# Make sure to update the ENTITY and WEAVE_PROJECT in config.py to the correct project!\n",
        "# Uncomment the following line to use your own annotated data after running chapter_01_generate_medical_data.ipynb\n",
        "# annotated_medical_data = weave.ref(\n",
        "#     f\"weave:///{ENTITY}/{WEAVE_PROJECT}/object/medical_data_annotations:latest\",\n",
        "# ).get()\n",
        "annotated_medical_data = weave.ref(\"weave:///a-sh0ts/eval_course_ch1_dev/object/medical_data_annotations:At9gri9UasftpPe5VNzT3EuIXQWAo5MYX8aMf2cuE8A\").get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcuuyG29gkQ3"
      },
      "outputs": [],
      "source": [
        "print_dialogue_data(annotated_medical_data, indexes_to_show=[0], max_chars=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSrnzCN8gkQ3"
      },
      "source": [
        "### In fact, let's just generate an example now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULufapcMgkQ3"
      },
      "outputs": [],
      "source": [
        "llm = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
        "llm.predict(\n",
        "    user_prompt=medical_task.format(transcript=annotated_medical_data[0][0][\"input\"]),\n",
        "    system_prompt=medical_system_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyPgulL1gkQ3"
      },
      "source": [
        "## Data Collection and Curation for Evaluation\n",
        "\n",
        "### Our approach for medical extraction evaluation data:\n",
        "\n",
        "1. Start with real medical transcripts from production systems\n",
        "   - Actual doctor-patient conversations\n",
        "   - Authentic medical terminology and flows\n",
        "   - Real-world edge cases\n",
        "\n",
        "2. Dataset Diversity Requirements\n",
        "   - Various medical conditions\n",
        "   - Different conversation styles\n",
        "   - Mix of routine and complex cases\n",
        "   - Remove duplicates for clean evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFsqZgBJgkQ3"
      },
      "outputs": [],
      "source": [
        "print_dialogue_data(annotated_medical_data, indexes_to_show=[1], max_chars=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__PVUUb8gkQ3"
      },
      "source": [
        "## Why and How to Evaluate LLMs\n",
        "\n",
        "### Core Principles of LLM Evaluation\n",
        "Unlike traditional software testing, LLM evaluation requires special consideration:\n",
        "\n",
        "1. **Non-Deterministic Outputs**\n",
        "   - Models can give different valid answers\n",
        "   - Responses vary between runs\n",
        "   - Multiple correct solutions possible\n",
        "\n",
        "2. **Quality is Multi-Dimensional**\n",
        "   - Correctness isn't binary\n",
        "   - Context matters heavily\n",
        "   - Different stakeholders have different priorities\n",
        "\n",
        "3. **Scale vs Accuracy Trade-offs**\n",
        "   - Manual review is accurate but expensive\n",
        "   - Automated checks are scalable but limited\n",
        "   - Hybrid approaches often work best\n",
        "\n",
        "### Practical Evaluation Recipe 🧑‍🍳\n",
        "\n",
        "1. **Define Success Criteria**\n",
        "   - List must-have requirements\n",
        "   - Set acceptable thresholds\n",
        "   - Identify critical failures\n",
        "\n",
        "2. **Build Evaluation Suite**\n",
        "   - Automated checks for clear rules\n",
        "   - Expert review for nuanced cases\n",
        "   - Version control evaluation code\n",
        "\n",
        "3. **Create Scoring System**\n",
        "   - Establish baselines\n",
        "\n",
        "### Applying to Medical Data Extraction 🏥\n",
        "\n",
        "For our medical extraction task, this means:\n",
        "- **Success Criteria**: Required fields, privacy compliance, word limits\n",
        "- **Evaluation Suite**: Automated checks + medical expert review\n",
        "- **Scoring**: Combination of format, accuracy, and safety metrics\n",
        "\n",
        "Let's see how to implement this..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2SLbUOngkQ3"
      },
      "source": [
        "![](https://github.com/wandb/eval-course/blob/main/notebooks/media/traditional_llm_eval.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muAjbUdwgkQ4"
      },
      "source": [
        "## Annotation: Building Quality Training Data\n",
        "\n",
        "### Why Annotate?\n",
        "To evaluate LLMs effectively, we need expert-labeled data that:\n",
        "1. Defines what \"good\" looks like\n",
        "2. Shows us what to test for\n",
        "3. Helps align our automated tests with human judgment\n",
        "\n",
        "### Ideal Process\n",
        "Experts review outputs and provide structured feedback. This creates a foundation for:\n",
        "- Building automated evaluation tests\n",
        "- Measuring how well those tests match expert judgment\n",
        "- Refining our evaluation methods until they align with expert standards\n",
        "\n",
        "### Our Annotation Process\n",
        "NOTE: In a production system, this would be done by licensed medical professionals using a strict rubric.\n",
        "For this example code, we'll use synthetic annotations to demonstrate the process:\n",
        "\n",
        "1. Binary Pass/Fail Judgments\n",
        "   - Pass: Correctly extracted key medical information\n",
        "   - Fail: Missed critical details or made dangerous assumptions\n",
        "\n",
        "2. Detailed Critiques Required\n",
        "   - For Passes: Document accuracy while noting improvement areas\n",
        "   - For Fails: Identify specific medical extraction errors and their potential impact\n",
        "\n",
        "These annotated examples become our evaluation dataset, though in practice,\n",
        "medical evaluations should always be validated by qualified healthcare professionals.\n",
        "\n",
        "Think of annotations as our compass - they help ensure our later automated evaluation methods point in the same direction as human experts while assessing the quality of our LLM's outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij2edJrPgkQ4"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/wandb/eval-course/blob/main/notebooks/media/annotation_ui.png?raw=1\" width=\"450\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCr-7y8KgkQ4"
      },
      "outputs": [],
      "source": [
        "print_dialogue_data(annotated_medical_data, indexes_to_show=[2, 3, 4], max_chars=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkiYtA3ZgkQ4"
      },
      "source": [
        "## Evaluation: Measuring Performance\n",
        "\n",
        "### Understanding LLM Evaluation\n",
        "Unlike traditional software testing, LLM evaluation requires multiple approaches:\n",
        "\n",
        "1. **Automated Checks**\n",
        "   - Fast, programmatic tests\n",
        "   - Clear pass/fail criteria\n",
        "   - Example: format rules, required fields\n",
        "\n",
        "2. **Model-Assisted Evaluation**\n",
        "   - Using LLMs to evaluate outputs\n",
        "   - Helpful for subjective criteria\n",
        "   - Example: checking medical accuracy, privacy compliance\n",
        "\n",
        "3. **Expert Review**\n",
        "   - Human validation of complex cases\n",
        "   - Ground truth for training evaluators\n",
        "   - Example: annotated datasets\n",
        "\n",
        "### Building Evaluation Systems\n",
        "\n",
        "In this notebook, we'll implement this through:\n",
        "\n",
        "1. **Basic Tests**\n",
        "   ```python\n",
        "   test_adheres_to_required_keys()\n",
        "   test_adheres_to_word_limit()\n",
        "   ```\n",
        "\n",
        "2. **LLM Judges**\n",
        "   ```python\n",
        "   judge_adheres_to_privacy_guidelines()\n",
        "   judge_overall_score()\n",
        "   ```\n",
        "\n",
        "3. **Key Questions**\n",
        "   - How closely do automated evaluations match human judgment?\n",
        "   - When do automated systems diverge from human experts?\n",
        "   - What makes a good evaluation system?\n",
        "\n",
        "These questions lead us to the concept of alignment - measuring how well our automated systems match human expectations and values. We'll explore practical ways to measure and improve this alignment after implementing our evaluation system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b3-2EDegkQ4"
      },
      "source": [
        "![](https://github.com/wandb/eval-course/blob/main/notebooks/media/eval_task_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRVgQBlPgkQ4"
      },
      "source": [
        "### Using Domain Knowledge to Build Evaluation Tests\n",
        "\n",
        "We'll create four key tests to evaluate our medical extraction outputs:\n",
        "\n",
        "1. **Required Fields Check**\n",
        "   - Verifies presence of essential medical fields\n",
        "   - E.g., \"Chief complaint\", \"Symptoms\", \"Follow-up instructions\"\n",
        "\n",
        "2. **Word Limit Check**\n",
        "   - Ensures output stays within 150-word limit\n",
        "   - Promotes concise, focused summaries\n",
        "\n",
        "3. **Privacy Guidelines Check**\n",
        "   - Uses LLM to detect any PII leakage\n",
        "   - Critical for medical data compliance\n",
        "\n",
        "4. **Overall Quality Score**\n",
        "   - LLM-based assessment of extraction quality\n",
        "   - Considers accuracy, completeness, and format\n",
        "\n",
        "These tests will be validated against our expert-annotated dataset to ensure they align with human judgment. This alignment process helps us understand how well our automated evaluation matches medical expert standards.\n",
        "\n",
        "Let's implement each test:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISvmNWs7gkQ4"
      },
      "source": [
        "#### Software Tests: Older and more rigid approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0fldK9NgkQ4"
      },
      "outputs": [],
      "source": [
        "test_output = annotated_medical_data[0][1][\"output\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK5xcF1qgkQ4"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def test_adheres_to_required_keys(model_output: str):\n",
        "    # Required medical keys\n",
        "    required_keys = [\n",
        "        \"Chief complaint\",\n",
        "        \"History of present illness\",\n",
        "        \"Physical examination\",\n",
        "        \"Symptoms\",\n",
        "        \"New medications with dosages\",\n",
        "        \"Follow-up instructions\",\n",
        "    ]\n",
        "\n",
        "    # Convert to lowercase for case-insensitive matching\n",
        "    output_lower = model_output.lower()\n",
        "\n",
        "    # Check if all required keys are present\n",
        "    for key in required_keys:\n",
        "        if key.lower() not in output_lower:\n",
        "            return int(False)\n",
        "\n",
        "    return int(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt_lX2hbgkQ5"
      },
      "outputs": [],
      "source": [
        "test_adheres_to_required_keys(test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoUb189NgkQ5"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def test_adheres_to_word_limit(model_output: str):\n",
        "    return int(len(model_output.split()) <= 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s45RM1bHgkQ5"
      },
      "outputs": [],
      "source": [
        "test_adheres_to_word_limit(test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6KPUWjEgkQ5"
      },
      "source": [
        "#### LLM Judges: Newer and more flexible approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6BXpmJ9gkQ5"
      },
      "outputs": [],
      "source": [
        "display_prompt(medical_privacy_system_prompt)\n",
        "display_prompt(medical_privacy_judge_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iPqHF8igkQ5"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def judge_adheres_to_privacy_guidelines(model_output: str):\n",
        "    llm = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
        "    response = llm.predict(\n",
        "        user_prompt=medical_privacy_judge_prompt.format(text=model_output),\n",
        "        system_prompt=medical_privacy_system_prompt,\n",
        "        schema=MedicalPrivacyJudgement,\n",
        "    )\n",
        "    try:\n",
        "        result = json.loads(response.text.strip(\"\\n\"))\n",
        "        return int(not result[0][\"contains_pii\"])\n",
        "    except:\n",
        "        return int(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M0JT9BUgkQ5"
      },
      "outputs": [],
      "source": [
        "judge_adheres_to_privacy_guidelines(test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nwnoGmJgkQ5"
      },
      "outputs": [],
      "source": [
        "display_prompt(medical_task_score_system_prompt)\n",
        "display_prompt(medical_task_score_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9levg79BgkQ5"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def judge_overall_score(model_output: str):\n",
        "    llm = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
        "    response = llm.predict(\n",
        "        user_prompt=medical_task_score_prompt.format(text=model_output),\n",
        "        system_prompt=medical_task_score_system_prompt,\n",
        "        schema=MedicalTaskScoreJudgement,\n",
        "    )\n",
        "    try:\n",
        "        result = json.loads(response.text.strip(\"\\n\"))\n",
        "        return int(result[0][\"score\"])\n",
        "    except:\n",
        "        return int(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFmCU0lVgkQ5"
      },
      "outputs": [],
      "source": [
        "judge_overall_score(test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRtTLQEKgkQ9"
      },
      "source": [
        "### We already have a dataset of annotated medical data. We can use our tests to evaluate the outputs of our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUWBoiplgkQ9"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def annotated_data_passthrough(input, output):\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt_lNlFSgkQ9"
      },
      "outputs": [],
      "source": [
        "evaluation_data = [\n",
        "    {\n",
        "        \"input\": annotated_row[0][\"input\"],\n",
        "        \"output\": annotated_row[1][\"output\"],\n",
        "        \"scores\": {\n",
        "            \"human_required_keys\": deserialized_row.presence_of_keys,\n",
        "            \"human_word_limit\": deserialized_row.word_count,\n",
        "            \"human_absence_of_PII\": deserialized_row.absence_of_PII,\n",
        "            \"human_overall_score\": annotated_row[2],\n",
        "        },\n",
        "    }\n",
        "    for annotated_row in annotated_medical_data\n",
        "    if (deserialized_row := deserialize_model(annotated_row[3], MainCriteria))\n",
        "][0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lIRmBKggkQ9"
      },
      "outputs": [],
      "source": [
        "# Create evaluation\n",
        "evaluation = weave.Evaluation(\n",
        "    dataset=evaluation_data,\n",
        "    scorers=[\n",
        "        test_adheres_to_required_keys,\n",
        "        test_adheres_to_word_limit,\n",
        "        judge_adheres_to_privacy_guidelines,\n",
        "        judge_overall_score,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "evals = asyncio.run(evaluation.evaluate(annotated_data_passthrough))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC_ufuTPgkQ-"
      },
      "source": [
        "### But do our test outputs adhere to the annotation expectations?\n",
        "\n",
        "We need to measure how well our automated evaluations match human judgment. We'll:\n",
        "\n",
        "1. **Measure Alignment**\n",
        "   - Compare automated test results with expert annotations using kappa scores\n",
        "   - Weight different aspects based on their importance (privacy, completeness, etc.)\n",
        "   - Find where automated tests disagree with human experts\n",
        "\n",
        "2. **Use These Results**\n",
        "   - Chapter 2 will focus on improving the LLM judges that show poor alignment\n",
        "   - We'll learn to refine prompts based on these alignment scores\n",
        "   - Build better evaluation systems by focusing on the weakest areas first\n",
        "\n",
        "These alignment measurements are crucial - they tell us which parts of our automated system need the most work, especially for critical aspects like privacy checks and medical accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8QBLViGgkQ-"
      },
      "outputs": [],
      "source": [
        "# Get the evaluation call id from the evaluation object which you can see in the URL above!\n",
        "# This line will break for you!\n",
        "eval_call_id = \"01944203-0c3f-7c92-a0dc-69e2d2f2df26\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD_w8v1wgkQ-"
      },
      "outputs": [],
      "source": [
        "df = get_evaluation_predictions(weave_client, eval_call_id)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhux8eKygkQ-"
      },
      "source": [
        "#### Software Tests: Minimal Alignment and hard to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coSwC5j0gkQ-"
      },
      "outputs": [],
      "source": [
        "kappa_scores = calculate_kappa_scores(df, tuple_columns=[\"required_keys\", \"word_limit\"])\n",
        "for metric, score in kappa_scores.items():\n",
        "    print(f\"{metric}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7J1Y1gogkQ-"
      },
      "source": [
        "#### LLM Judges: Higher Alignment and easier to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsSETiwXgkQ-"
      },
      "outputs": [],
      "source": [
        "kappa_scores = calculate_kappa_scores(df, tuple_columns=[\"privacy\", \"overall\"])\n",
        "for metric, score in kappa_scores.items():\n",
        "    print(f\"{metric}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXIkXa9zgkQ-"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19U1TnafgkQ-"
      },
      "source": [
        "- [Hamel's LLM Judge](https://hamel.dev/blog/posts/llm-judge/)\n",
        "- [Hamel's LLM Evaluation](https://hamel.dev/blog/posts/evals/)\n",
        "- [Clef's LLM Evaluation](https://huggingface.co/blog/clefourrier/llm-evaluation)\n",
        "- [Eugene Yan's LLM Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n",
        "- [Shreya's AI Engineering Flywheel](https://www.sh-reya.com/blog/ai-engineering-flywheel/)\n",
        "- [Who Validates the Validators?](https://arxiv.org/abs/2404.12272)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}